{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import autojit, prange\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "import hyperopt\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import roc_auc_score,auc,roc_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from collections import Counter\n",
    "import sys\n",
    "from os.path import dirname\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "dff=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group both train and test to make a combined network\n",
    "df=df[['node1_id','node2_id']].append(dff[['node1_id','node2_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes from merged dataframe to initialized graph\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(df.node1_id)\n",
    "g.add_nodes_from(df.node2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add edges to graph\n",
    "edges = list(df[['node1_id', 'node2_id']].to_records(index=False))\n",
    "g.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get degree to calculate nodes\n",
    "print('Number of unique questions:', len(set(df.node1_id) | set(df.node2_id)), g.number_of_nodes())\n",
    "print('Number of rows in the data:', len(df), g.number_of_edges())\n",
    "\n",
    "d = g.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to store neighbour count for every node\n",
    "dd={}\n",
    "for k in d:\n",
    "    dd[k[0]]=k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create number of neighbours feature for both nodes\n",
    "comb = pd.DataFrame()\n",
    "comb['node1_neighbor_count'] = df['node1_id'].apply(lambda x:dd[x])\n",
    "comb['node2_neighbor_count'] = df['node2_id'].apply(lambda x:dd[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add difference between neighbours of nodes as a feature\n",
    "comb['diff']=abs(comb['node1_neighbor_count']-comb['node2_neighbor_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ratio of number of neighbours of both nodes\n",
    "comb['div']=(comb['node1_neighbor_count']/comb['node2_neighbor_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb['node1_id']=df['node1_id']\n",
    "comb['node2_id']=df['node2_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common neighbours\n",
    "def get_intersection_count(row):\n",
    "    return(len(set(g.neighbors(row.node1_id)).intersection(set(g.neighbors(row.node2_id)))))\n",
    "start=time.time()\n",
    "comb['common']=df.apply(lambda row: get_intersection_count(row), axis=1)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pagerank features\n",
    "start=time.time()\n",
    "pg=nx.pagerank(g)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "comb['pr_node1'] = comb.apply(lambda row: pg[row.node1_id], axis=1)\n",
    "comb['pr_node2'] = comb.apply(lambda row: pg[row.node2_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clustering features\n",
    "start=time.time()\n",
    "cl=nx.clustering(g)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "comb['cluster_node1'] = comb.apply(lambda row: cl[row.node1_id], axis=1)\n",
    "comb['cluster_node2'] = comb.apply(lambda row: cl[row.node2_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get resource allocation index to use as a feature\n",
    "ra=nx.resource_allocation_index(g,edges)\n",
    "lss=[]\n",
    "@autojit\n",
    "def parallel_sum(A,parallel=True):\n",
    "    for i in ad:\n",
    "        lss.append(i[2])\n",
    "\n",
    "    return lss\n",
    "start=time.time()\n",
    "parallel_sum(ra)\n",
    "end=time.time()\n",
    "print(end-start) \n",
    "comb['ra']=lss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate shortest path between nodes\n",
    "ls=[]\n",
    "start=time.time()\n",
    "for j in edges:\n",
    "    ls.append(nx.shortest_path_length(g,j[0],j[1]))\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "comb['dist']=ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average degree for every node\n",
    "start=time.time()\n",
    "avgdegree=nx.average_neighbor_degree(g)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "comb['avgdeg_node1'] = comb.apply(lambda row: avgdegree[row.node1_id], axis=1)\n",
    "comb['avgdeg_node2'] = comb.apply(lambda row: avgdegree[row.node2_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate centrality for every node\n",
    "start=time.time()\n",
    "cen=nx.degree_centrality(g)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "comb['degcen_node1'] = comb.apply(lambda row: cen[row.node1_id], axis=1)\n",
    "comb['degcen_node2'] = comb.apply(lambda row: cen[row.node2_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some more neighbour based features based on existing literature on network analysis\n",
    "comb['mul']=comb['node1_neighbor_count']*comb['node2_neighbor_count']\n",
    "comb['totalfriends']=comb['node1_neighbor_count']+comb['node2_neighbor_count']-2*comb['common']\n",
    "comb['jaccard']=comb['common']/comb['totalfriends']\n",
    "comb['SI']=comb['common']/(comb['node1_neighbor_count']+comb['node2_neighbor_count'])\n",
    "comb['SC']=comb['common']/np.sqrt((comb['node1_neighbor_count']*comb['node2_neighbor_count']))\n",
    "comb['HP']=comb['common']/np.minimum(comb['node1_neighbor_count'],comb['node2_neighbor_count'])\n",
    "comb['HD']=comb['common']/np.maximum(comb['node1_neighbor_count'],comb['node2_neighbor_count'])\n",
    "comb['PD']=comb['common']/comb['mul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features based on user chat history\n",
    "df=pd.read_csv('user_features.csv')\n",
    "dff=pd.read_csv('train.csv')\n",
    "prep1=pd.merge(df, dff, left_on='node_id', right_on='node1_id')\n",
    "df1 = prep1.groupby(by = ['node1_id'])['is_chat'].agg(['sum','count']).reset_index()\n",
    "df1.columns = ['node_id',\"chat\",\"conn\"]\n",
    "df2 = prep1.groupby(by = ['node2_id'])['is_chat'].agg(['sum','count']).reset_index()\n",
    "df2.columns = ['node_id',\"chat\",\"conn\"]\n",
    "final = pd.concat([df1,df2],axis=0)\n",
    "final = final.groupby(by = ['node_id']).sum().reset_index()\n",
    "final['chat_conn'] = final.chat/final.conn\n",
    "final['chat_activity'] = np.where(final.chat >3,final.chat_conn,0)\n",
    "final['chat_cnt'] = np.where(final.chat >3,final.chat,0)\n",
    "prepped_feat=final[['node_id','chat_activity','chat_cnt']]\n",
    "userfeat=df.merge(prepped_feat,on='node_id',how='outer')\n",
    "userfeat.to_csv('user_features_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep\n",
    "train=pd.read_csv('train.csv')\n",
    "user_feats=pd.read_csv('user_features_new.csv')\n",
    "prep1=pd.merge(train, user_feats, left_on='node1_id', right_on='node_id')\n",
    "prep2=pd.merge(prep1, user_feats, left_on='node2_id', right_on='node_id')\n",
    "combo_tr=combo[0:train.shape[0]]\n",
    "fin=combo_tr.merge(prep2,on=['node1_id','node2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance classes\n",
    "train_pos = fin[fin.is_chat==1]\n",
    "train_neg = fin[fin.is_chat!=1]\n",
    "train_neg['r'] = np.random.rand(len(train_neg))\n",
    "train_neg = train_neg[train_neg.r <= train_pos.shape[0]/fin.shape[0]]\n",
    "train_neg.drop(columns = ['r'], inplace = True)\n",
    "train_data = pd.concat([train_pos,train_neg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out target variable\n",
    "y=train_data['is_chat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameters\n",
    "params = {\n",
    "            'verbose_eval': True,\n",
    "            \"objective\":\"binary\",\n",
    "        'device':'cpu',\n",
    "        \"boosting\":\"gbdt\",\n",
    "    'boost_from_average' : False\n",
    "    }\n",
    "# lgtrain = lgb.Dataset(df, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features to be used\n",
    "cll=['f1_x', 'f2_x', 'f3_x',\n",
    "       'f4_x', 'f5_x', 'f6_x', 'f7_x', 'f8_x', 'f9_x', 'f10_x', 'f11_x',\n",
    "       'f12_x', 'f13_x', 'f1_y', 'f2_y', 'f3_y', 'f4_y', 'f5_y',\n",
    "       'f6_y', 'f7_y', 'f8_y', 'f9_y', 'f10_y', 'f11_y', 'f12_y', 'f13_y','node1_neighbor_count', 'node2_neighbor_count','diff','div','dist','common','mul','totalfriends','jaccard','SI','SC','HP','HD','PD','cluster_node1','cluster_node2','pr_node1','pr_node2','ra','avgdeg_node1',\n",
    "       'avgdeg_node2', 'degcen_node1', 'degcen_node2','chat_activity_x','chat_activity_y','chat_cnt_x','chat_cnt_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final prepared training set\n",
    "data=train_data[cll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categorical features\n",
    "cat_feat=[ 'f13_x','f13_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize train set via lgb\n",
    "lgtrain = lgb.Dataset(data, label=y,categorical_feature=cat_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "cv = lgb.cv(params,\n",
    "                lgtrain,\n",
    "                nfold=5,metrics='auc',\n",
    "                num_boost_round=1500,\n",
    "                early_stopping_rounds=50,stratified=True,shuffle=True,verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model\n",
    "model = lgb.train(params,lgtrain,num_boost_round=1250,verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data prep\n",
    "test=pd.read_csv('test.csv')\n",
    "prep1_test=pd.merge(test, user_feats, left_on='node1_id', right_on='node_id')\n",
    "prep2_test=pd.merge(prep1_test, user_feats, left_on='node2_id', right_on='node_id')\n",
    "combo_te=combo[train.shape[0]:]\n",
    "fin_test=combo_te.merge(prep2_test,on=['node1_id','node2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out ids\n",
    "ids=fin_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "preds=model.predict(fin_test[cll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for submission\n",
    "op=pd.DataFrame()\n",
    "op['id']=ids\n",
    "op['is_chat']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.to_csv('pr.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
